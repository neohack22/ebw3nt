{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/ebw3nt/blob/main/Correction_lab_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl9nmloVpySE"
      },
      "source": [
        "L'objectif est de mettre en place un classificateur simple pour l'analyse de texte et de sentiments. La t√¢che consiste en la classification binaire des critiques de films. Le jeu de donn√©es utilis√© fait partie du jeu de donn√©es *imdb*. Vous pouvez trouver le jeu de donn√©es original sur le site web d'*[imdb](https://www.imdb.com/interfaces/)* ou une version sur le site web de [Kaggle](https://www.kaggle.com/utathya/imdb-review-dataset). Pour ce TP, nous utiliserons une version pr√©trait√©e.\n",
        "\n",
        "D√©roulement¬†:\n",
        "\n",
        "- Charger, nettoyer et configurer les donn√©es (√©tape cruciale en pratique, mais nous l'ignorons pour ce TP).\n",
        "\n",
        "- Pr√©parer les donn√©es pour les mod√®les PyTorch.\n",
        "\n",
        "- D√©finir votre propre mod√®le.\n",
        "\n",
        "- Exp√©rimentations.\n",
        "\n",
        "# Les donn√©es\n",
        "\n",
        "Les jeux de donn√©es sont disponibles dans le d√©p√¥t cloud. Il y a deux fichiers¬†: un pour les critiques positives (imdb.pos) et un pour les critiques n√©gatives (imdb.neg). Chaque classe contient 300¬†000 exemples.\n",
        "\n",
        "Voici deux fonctions pour charger et nettoyer les donn√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aLVRPoUpySK",
        "outputId": "c5a27f1c-3e71-496e-b322-32c359f89ce9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f57a2a39350>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "th.manual_seed(1) # set the seed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports essentiels\n",
        "\n",
        "* **re** : traitement de texte (regex)\n",
        "* **numpy** : op√©rations num√©riques rapides\n",
        "* **torch** : cr√©ation et entra√Ænement de mod√®les\n",
        "* **torch.autograd** : calcul automatique des gradients\n",
        "* **torch.nn / torch.nn.functional** : couches et fonctions de r√©seaux\n",
        "* **random / math** : fonctions utilitaires\n",
        "* **pickle / gzip** : chargement & sauvegarde de donn√©es\n",
        "\n",
        "Ces imports pr√©parent tout le pipeline :\n",
        "> **manipuler les donn√©es ‚Üí d√©finir un mod√®le ‚Üí l‚Äôentra√Æner ‚Üí sauvegarder.**\n",
        "\n",
        "---\n",
        "\n",
        "### Fixer le seed\n",
        "\n",
        "```python\n",
        "th.manual_seed(1)\n",
        "```\n",
        "\n",
        " : garantir la reproductibilit√©\n",
        "\n",
        "* m√™mes poids initiaux\n",
        "* m√™mes r√©sultats d‚Äôun run √† l‚Äôautre\n",
        "* plus facile √† d√©boguer\n",
        "* indispensable pour les projets collaboratifs"
      ],
      "metadata": {
        "id": "7_BEpCYuvGc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ TensorFlow vs PyTorch : diff√©rences conceptuelles\n",
        "\n",
        "### 1) **Paradigme principal**\n",
        "\n",
        "| Aspect      | TensorFlow                                                                             | PyTorch                                                                             |\n",
        "| ----------- | -------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| Type        | Graphes statiques (TF1.x) / Graphes dynamiques (TF2.x avec Eager Execution)            | Graphes dynamiques (Define-by-Run)                                                  |\n",
        "| Implication | Le graphe est d√©fini **avant** l‚Äôex√©cution ‚Üí optimisation possible mais moins intuitif | Le graphe se construit **au moment de l‚Äôex√©cution** ‚Üí plus intuitif, facile √† debug |\n",
        "\n",
        "* TensorFlow classique : ‚Äúon planifie l‚Äôitin√©raire avant de partir‚Äù\n",
        "* PyTorch : ‚Äúon construit la route au fur et √† mesure qu‚Äôon avance‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### 2) **D√©bogage et lisibilit√©**\n",
        "\n",
        "* **PyTorch** : simple, syntaxe Python native ‚Üí les erreurs sont imm√©diates, facile √† utiliser pour exp√©rimenter.\n",
        "* **TensorFlow** : TF1.x n√©cessitait des sessions et placeholders ‚Üí plus compliqu√© √† debug, TF2.x am√©liore beaucoup avec `Eager Execution`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) **API et flexibilit√©**\n",
        "\n",
        "* **PyTorch** : API plus ‚ÄúPythonique‚Äù, tr√®s proche du code scientifique classique (numpy-like).\n",
        "* **TensorFlow** : API plus rigide au d√©part, mais tr√®s modulable via Keras et TF Hub pour d√©ployer facilement en production.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) **D√©ploiement et production**\n",
        "\n",
        "* **TensorFlow** : meilleur pour la production et les mod√®les mobiles / web (TensorFlow Lite, TensorFlow.js).\n",
        "* **PyTorch** : historquement plus orient√© recherche, mais avec TorchScript et PyTorch Lightning, le d√©ploiement devient facile.\n",
        "\n",
        "---\n",
        "\n",
        "### 5) **Communaut√© et adoption**\n",
        "\n",
        "* **PyTorch** : tr√®s populaire dans la recherche acad√©mique et exp√©rimentations rapides.\n",
        "* **TensorFlow** : tr√®s utilis√© dans l‚Äôindustrie et les solutions commerciales, notamment pour l‚ÄôIA int√©gr√©e et les produits Google.\n",
        "\n",
        "> PyTorch = intuitif, flexible, id√©al pour exp√©rimenter et prototyper.\n",
        "> TensorFlow = industrialisable, modulable, id√©al pour d√©ployer √† grande √©chelle."
      ],
      "metadata": {
        "id": "SiFJfbsg1TEK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ucjfrVbpySN"
      },
      "source": [
        "# Chargement des donn√©es\n",
        "\n",
        "Charger les donn√©es :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K59vHLBpySN",
        "outputId": "745ce76b-1536-4f21-fc3a-b3db55efd4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-06 11:30:46--  https://drive.google.com/uc?export=download&id=1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.18.206, 2a00:1450:4007:812::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.18.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n2hk1hevtcvg0o0qrmu5u6kngpg8htco/1649237400000/16692574002775380562/*/1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-06 11:30:50--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n2hk1hevtcvg0o0qrmu5u6kngpg8htco/1649237400000/16692574002775380562/*/1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA?e=download\n",
            "Resolving doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)... 216.58.213.129, 2a00:1450:4007:811::2001\n",
            "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|216.58.213.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1552309 (1,5M) [application/x-gzip]\n",
            "Saving to: ‚Äòimdb.pck.gz‚Äô\n",
            "\n",
            "imdb.pck.gz         100%[===================>]   1,48M  5,08MB/s    in 0,3s    \n",
            "\n",
            "2022-04-06 11:30:51 (5,08 MB/s) - ‚Äòimdb.pck.gz‚Äô saved [1552309/1552309]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# find the file imdb.pck.gz, and set the next variable accordingly\n",
        "filename = 'imdb.pck.gz'\n",
        "\n",
        "# You can download the file with the following line:\n",
        "! wget \"https://drive.google.com/uc?export=download&id=1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA\" -O imdb.pck.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**On t√©l√©charge automatiquement le fichier de donn√©es et on le sauvegarde sous le nom `imdb.pck.gz`.**\n",
        "\n",
        "* Le fichier s‚Äôappelle : `imdb.pck.gz`\n",
        "* `.gz` ‚Üí signifie que le fichier est **compress√©** (comme un ZIP).\n",
        "* `.pck` ‚Üí signifie qu‚Äôil contient des **donn√©es Python s√©rialis√©es** (stock√©es pour √™tre r√©ouvertes plus tard).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "! wget \"https://drive.google.com/uc?export=download&id=1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA\" -O imdb.pck.gz\n",
        "```\n",
        "\n",
        "est une commande ex√©cut√©e **depuis Python**, mais c‚Äôest en r√©alit√© une instruction syst√®me :\n",
        "\n",
        "* `wget` ‚Üí un outil pour t√©l√©charger un fichier depuis internet\n",
        "* l‚ÄôURL ‚Üí pointe vers un fichier stock√© sur Google Drive\n",
        "* `-O imdb.pck.gz` ‚Üí on choisit le **nom du fichier** que l‚Äôon veut obtenir localement"
      ],
      "metadata": {
        "id": "KDlNFuA8wADR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hVFv-0u_pySO"
      },
      "source": [
        "Ouvrez les donn√©es avec Python et vous obtiendrez trois objets¬†:\n",
        "\n",
        "- *texts*¬†: une liste de tenseurs, chaque tenseur repr√©sentant une s√©quence de mots √† classifier.\n",
        "\n",
        "- *labels*¬†: la classe (positive ou n√©gative) du texte correspondant.\n",
        "\n",
        "- *lexicon*¬†: un dictionnaire associant les entiers aux mots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlMFMQ08pySO",
        "outputId": "d52cb691-861d-4f36-c49f-269a8c45afed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'torch.Tensor'> <class 'dict'>\n",
            "tensor([ 36,  25, 381,  10,  58,  21,  83])\n",
            "nb examples :  30000\n",
            "Vocab size:  5002\n"
          ]
        }
      ],
      "source": [
        "fp = gzip.open(filename,'rb')\n",
        "texts , labels, lexicon  = pickle.load(fp)\n",
        "\n",
        "print(type(texts), type(labels), type(lexicon))\n",
        "print(texts[0])\n",
        "print(\"nb examples : \", len(texts))\n",
        "VOCAB_SIZE = len(lexicon)\n",
        "print(\"Vocab size: \", VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code lit un fichier compress√© contenant des donn√©es Python pr√©par√©es pour du traitement de texte ou du machine learning. On r√©cup√®re les textes, leurs √©tiquettes, et le lexique pour pouvoir ensuite les utiliser dans un mod√®le :\n",
        "\n",
        "1. **Ouverture du fichier compress√©**\n",
        "\n",
        "* `gzip.open` ouvre un fichier `.gz` en mode lecture binaire (`'rb'`).\n",
        "* Le fichier contient des donn√©es Python sauvegard√©es pr√©c√©demment avec `pickle`.\n",
        "\n",
        "2. **Chargement des objets Python**\n",
        "\n",
        "* `pickle.load(fp)` **d√©compresse et restaure** les objets Python contenus dans le fichier.\n",
        "* Ici, on r√©cup√®re trois objets :\n",
        "\n",
        "  * `texts` : la liste ou tableau de textes (ex. phrases, documents‚Ä¶)\n",
        "  * `labels` : les √©tiquettes correspondantes (ex. cat√©gories ou classes)\n",
        "  * `lexicon` : le dictionnaire ou liste de mots connus dans le corpus\n",
        "\n",
        "3. **Exploration des types et du contenu**\n",
        "\n",
        "* On v√©rifie **le type des objets** pour savoir si ce sont des listes, des tableaux, etc.\n",
        "* On affiche **le premier texte** (`texts[0]`) pour voir un exemple.\n",
        "* On calcule le **nombre d‚Äôexemples** (`len(texts)`) et la **taille du vocabulaire** (`len(lexicon)`)."
      ],
      "metadata": {
        "id": "mdlliin7yGmY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBHrK2okpySP"
      },
      "source": [
        "Notez qu'un nombre r√©duit de mots est s√©lectionn√© pour constituer le vocabulaire. Les mots les moins fr√©quents sont √©cart√©s et remplac√©s par une forme sp√©cifique (*unk* pour inconnu)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a une liste de mots appel√©e lexicon et on souhaite parcourir les 10 premiers √©l√©ments pour les afficher avec leur position dans la liste."
      ],
      "metadata": {
        "id": "1EcDMb9dzQy5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SlDfDuVpySQ",
        "outputId": "92b5a3ea-7909-44db-d2ec-4e8b3913cc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word of index 0  :  <pad>\n",
            "word of index 1  :  <unk>\n",
            "word of index 2  :  !\n",
            "word of index 3  :  the\n",
            "word of index 4  :  a\n",
            "word of index 5  :  of\n",
            "word of index 6  :  movie\n",
            "word of index 7  :  and\n",
            "word of index 8  :  this\n",
            "word of index 9  :  to\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(\"word of index\", i , \" : \", lexicon[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08EvO2IfpySR"
      },
      "source": [
        "Pour lire le texte, vous pouvez utiliser par exemple le code suivant¬†:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUNU4tIdpySR",
        "outputId": "26aaa4f1-ed60-496f-f151-baedc638f5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7])\n",
            "Some positive reviews\n",
            "------------\n",
            "['strong', 'drama']\n",
            "['please', 'remake', 'this', 'movie']\n",
            "['very', 'funny', '!']\n",
            "['great', 'series']\n",
            "['fun', 'movie']\n",
            "Some negative reviews\n",
            "------------\n",
            "['absolute', 'waste', 'of', 'time']\n",
            "['the', 'worst', 'movie', 'ever', 'made']\n",
            "['slow', 'motion', 'picture', 'that', 'did', \"n't\", 'get', 'to', 'the', 'point']\n",
            "['there', 'are', 'good', 'bad', 'movies', 'and', 'there', 'are', 'bad', 'bad', 'movies', 'this', 'one', 'is', 'a', 'real', 'stinker']\n",
            "['<unk>', 'so', 'bad', 'its', 'funny']\n",
            "-----------\n",
            "A random sentence: \n",
            "['you', 'definitely', 'need', 'to', 'see', 'this', 'movie']\n"
          ]
        }
      ],
      "source": [
        "def idx2wordlist(idx_array,lexicon):\n",
        "    l = []\n",
        "    for i in idx_array:\n",
        "        l.append(lexicon[i.item()])\n",
        "    return l\n",
        "print(texts[0].shape)\n",
        "print(\"Some positive reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5):\n",
        "    print(idx2wordlist(texts[i+50],lexicon))\n",
        "print(\"Some negative reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5):\n",
        "    print(idx2wordlist(texts[-i-2000],lexicon))\n",
        "\n",
        "print(\"-----------\\nA random sentence: \")\n",
        "print(idx2wordlist(texts[104],lexicon))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque mot du texte est repr√©sent√© par un **entier** dans `texts`. Pour rendre le texte lisible, il faut **convertir ces indices en mots**, en utilisant un dictionnaire appel√© `lexicon`.\n",
        "\n",
        "---\n",
        "\n",
        "`idx2wordlist` : transformer une s√©quence d‚Äôindices (`idx_array`) en une liste de mots lisibles.\n",
        "* **Param√®tres** :\n",
        "\n",
        "  * `idx_array` : tableau d‚Äôindices repr√©sentant les mots d‚Äôune phrase ou d‚Äôun texte.\n",
        "  * `lexicon` : dictionnaire ou liste o√π chaque position correspond √† un mot.\n",
        "* **Fonctionnement** :\n",
        "\n",
        "  1. On cr√©e une liste vide `l`.\n",
        "  2. Pour chaque indice `i` dans le tableau :\n",
        "\n",
        "     * `i.item()` r√©cup√®re la valeur enti√®re si c‚Äôest un objet tensor.\n",
        "     * `lexicon[i.item()]` renvoie le mot correspondant.\n",
        "     * On l‚Äôajoute √† la liste `l`.\n",
        "  3. La fonction retourne la liste de mots.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(texts[0].shape)\n",
        "```\n",
        "\n",
        "* Affiche la **taille du premier texte**. Cela permet de v√©rifier combien de mots il contient.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(\"Some positive reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5):\n",
        "    print(idx2wordlist(texts[i+50], lexicon))\n",
        "```\n",
        "\n",
        "* On affiche **5 critiques positives**.\n",
        "* `texts[i+50]` : on choisit des textes √† partir de l‚Äôindice 50, suppos√©s positifs.\n",
        "* `idx2wordlist(...)` convertit les indices en mots pour qu‚Äôon puisse lire le texte.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(\"Some negative reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5):\n",
        "    print(idx2wordlist(texts[-i-2000], lexicon))\n",
        "```\n",
        "\n",
        "* Ici, on affiche **5 critiques n√©gatives**.\n",
        "* L‚Äôindice `-i-2000` permet de **prendre des textes depuis la fin** du tableau, suppos√©s n√©gatifs.\n",
        "* M√™me conversion en mots avec `idx2wordlist`.\n",
        "\n",
        "---\n",
        "\n",
        "Afficher une phrase al√©atoire :\n",
        "\n",
        "```python\n",
        "print(\"-----------\\nA random sentence: \")\n",
        "print(idx2wordlist(texts[104], lexicon))\n",
        "```\n",
        "\n",
        "Simplement, on choisit le texte √† l‚Äôindice 104 pour le lire."
      ],
      "metadata": {
        "id": "aUj_DFHMzeCL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlcwqfQpySS"
      },
      "source": [
        "# Interface donn√©es/mod√®le\n",
        "\n",
        "En pratique, nous partons de textes bruts que nous devons convertir en indices de mots. √Ä cette √©tape, nous pouvons effectuer le pr√©traitement du texte, la tokenisation et le nettoyage des donn√©es. Dans le cas pr√©sent, c'est d√©j√† fait. Mais en pratique, c'est une √©tape cruciale.\n",
        "\n",
        "L'objectif est d'impl√©menter un classificateur CBOW (Continuous Bag of Words, ou sac d'embeddings de mots). Cela signifie que la premi√®re couche du mod√®le traite les embeddings de mots.\n",
        "\n",
        "Le module **Embedding** de PyTorch est con√ßu √† cet effet. Ce module attend en entr√©e un tableau ou une liste d'indices de mots. Pour cette session, l'objectif est de d√©velopper rapidement un mod√®le. L'interface de donn√©es est donc assez simple.\n",
        "\n",
        "Nous terminons cette section par la cr√©ation de labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBwbWCEwpyST"
      },
      "source": [
        "# Un premier mod√®le\n",
        "\n",
        "Le premier mod√®le est un CBOW (Continuous Bag of Words). Un texte est repr√©sent√© comme un ensemble de mots (un sac de caract√©ristiques binaires)¬†:\n",
        "\n",
        "- Chaque mot est associ√© √† son plongement.\n",
        "\n",
        "- Le texte est repr√©sent√© comme la somme des plongements des mots impliqu√©s.\n",
        "\n",
        "- Cette somme de plongements est ensuite transmise √† une couche lin√©aire √† une seule unit√© de sortie,\n",
        "\n",
        "- suivie d'une fonction d'activation sigmo√Øde. La sortie du mod√®le est similaire √† une r√©gression logistique.\n",
        "\n",
        "Nous souhaitons maintenant impl√©menter ce mod√®le en PyTorch. Une approche possible consiste √† commencer par construire ce mod√®le (ou un exemple simple mais similaire) **√©tape par √©tape**, puis √† cr√©er une classe permettant d'encapsuler le tout dans un **mod√®le**.\n",
        "\n",
        "## Construction du mod√®le, √©tape par √©tape\n",
        "\n",
        "La couche d'entr√©e du mod√®le est une couche de plongement. Celle-ci est d√©j√† impl√©ment√©e en PyTorch. Consultez l'exemple simple suivant¬†:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q25KzwFlpySU",
        "outputId": "cd77d9d1-f5d7-45e7-d60c-980e4d6ce137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The input:  tensor([ 21, 316, 320,   9,  59,   8,   6])\n",
            "length:  7\n",
            "Embs shape :  torch.Size([7, 4])\n",
            "tensor([[ 0.6971, -0.9576, -1.0220,  1.3295],\n",
            "        [ 1.0256,  1.7889, -1.2001,  0.8268],\n",
            "        [-1.1081,  0.4350, -0.5725, -1.6943],\n",
            "        [-0.9530, -1.2833, -0.6837,  1.3832],\n",
            "        [ 0.2081, -0.4403,  1.3717,  0.9725],\n",
            "        [-0.5415, -1.4216, -0.0367, -1.9919],\n",
            "        [-1.3417,  0.0124, -1.3485, -0.5739]], grad_fn=<EmbeddingBackward>)\n"
          ]
        }
      ],
      "source": [
        "# build an Embedding layer\n",
        "# it is important to understand the parameters given to the constructor !\n",
        "D = 4\n",
        "embLayer = th.nn.Embedding(num_embeddings=len(lexicon), embedding_dim=D)\n",
        "# The dim of 4 is a toy example.\n",
        "# run forward on some input\n",
        "inp = texts[104]\n",
        "embs = embLayer(inp) # embLayer.forward(inp)\n",
        "# Look at the dimension of i/o\n",
        "print(\"The input: \",inp)\n",
        "print(\"length: \",len(inp))\n",
        "print(\"Embs shape : \",embs.shape)\n",
        "print(embs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une **couche d‚ÄôEmbedding** transforme des **entiers repr√©sentant des mots** en **vecteurs de nombres r√©els**.\n",
        "Ces vecteurs sont utiles pour les mod√®les de deep learning car ils repr√©sentent les mots de fa√ßon **dense et continue** (contrairement aux repr√©sentations one-hot qui sont tr√®s grandes et clairsem√©es).\n",
        "\n",
        "---\n",
        "\n",
        "üí° Plus la dimension est grande, plus les vecteurs peuvent capturer de **subtilit√©s s√©mantiques** des mots :\n",
        "\n",
        "* `num_embeddings` : le **nombre total de mots diff√©rents** que vous avez (ici `len(lexicon)`).\n",
        "* `embedding_dim` : la **dimension du vecteur** qui repr√©sentera chaque mot (ici `D=4` pour un exemple simple).\n",
        "\n",
        "---\n",
        "\n",
        "* `inp` est un **tableau d‚Äôindices** correspondant aux mots d‚Äôun texte.\n",
        "* `embLayer(inp)` transforme **chaque indice en vecteur de dimension `D`**.\n",
        "* Le r√©sultat `embs` est donc une **matrice** :\n",
        "\n",
        "  * **Lignes = nombre de mots dans le texte**\n",
        "  * **Colonnes = dimension des embeddings (`D`)**\n",
        "\n",
        "---\n",
        "\n",
        "* Si votre texte a 10 mots et que `D=4`, alors `embs.shape` sera `(10, 4)` :\n",
        "  chaque mot ‚Üí vecteur de 4 nombres.\n",
        "\n",
        "---\n",
        "\n",
        "* Les embeddings permettent au mod√®le de comprendre des relations entre mots.\n",
        "  Exemple : ¬´ roi ¬ª - ¬´ homme ¬ª + ¬´ femme ¬ª ‚âà ¬´ reine ¬ª dans des embeddings bien entra√Æn√©s.\n",
        "* M√™me pour un exemple p√©dagogique avec `D=4`, on voit comment **chaque mot devient un vecteur** utilisable dans un r√©seau de neurones."
      ],
      "metadata": {
        "id": "KAozY_lk3h1L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAo2OntepySV"
      },
      "source": [
        "Nous souhaitons maintenant compresser le tenseur r√©sultant selon la dimension temporelle. Cette dimension d√©pend des textes d'entr√©e, tandis que nous voulons construire une repr√©sentation de taille fixe de la phrase. La somme constitue une premi√®re piste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9aqQ9aJpySV",
        "outputId": "26197125-7c58-40e5-9456-ba7270543fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "## compute the sum of out to create a vector of size \"embedding_dim\".\n",
        "## Of course it will be a tensor with one dimension set to \"embedding_dim\".\n",
        "sumOfEmbs = embs.sum(dim=1)\n",
        "print(sumOfEmbs.shape) # check the shape\n",
        "sumOfEmbs = embs.sum(dim=0)\n",
        "print(sumOfEmbs.shape) # check the shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On veut **calculer la somme** de ces vecteurs selon diff√©rentes dimensions pour obtenir un nouveau vecteur ou une nouvelle structure.\n",
        "\n",
        "---\n",
        "\n",
        "1. **Somme sur la dimension 1**\n",
        "\n",
        "* `dim=1` signifie qu‚Äôon **additionne les √©l√©ments le long de la deuxi√®me dimension** du tensor (Python commence √† compter √† 0).\n",
        "* Si `embs` a la forme `(batch_size, embedding_dim)` :\n",
        "\n",
        "  * Chaque ligne correspond √† un embedding.\n",
        "  * La somme sur `dim=1` additionne tous les √©l√©ments **dans chaque vecteur**, ce qui donne **un scalaire par vecteur**.\n",
        "* R√©sultat : `sumOfEmbs` aura la forme `(batch_size,)`, un vecteur 1D avec une somme par ligne.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Somme sur la dimension 0**\n",
        "\n",
        "* `dim=0` signifie qu‚Äôon **additionne les √©l√©ments le long de la premi√®re dimension** du tensor.\n",
        "* Chaque colonne correspond √† une composante d‚Äôembedding sp√©cifique.\n",
        "* Additionner sur `dim=0` revient √† **faire la somme des embeddings ligne par ligne**, et obtenir un vecteur de taille `embedding_dim`.\n",
        "* R√©sultat : `sumOfEmbs` aura la forme `(embedding_dim,)`.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualisation simple\n",
        "\n",
        "Si `embs` est :\n",
        "\n",
        "```\n",
        "[[1, 2, 3],\n",
        " [4, 5, 6]]\n",
        "```\n",
        "\n",
        "* `embs.sum(dim=1)` ‚Üí `[6, 15]`\n",
        "  (somme de chaque ligne)\n",
        "\n",
        "* `embs.sum(dim=0)` ‚Üí `[5, 7, 9]`\n",
        "  (somme de chaque colonne)\n",
        "\n",
        "---\n",
        "\n",
        "* `dim=0` ‚Üí somme **verticale**, conserve la dimension des colonnes (embedding_dim).\n",
        "* `dim=1` ‚Üí somme **horizontale**, r√©duit chaque vecteur √† un scalaire."
      ],
      "metadata": {
        "id": "2tAOh1QTCh6S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fray2Zj_pySW"
      },
      "source": [
        "La derni√®re couche est une transformation lin√©aire¬†: l‚Äôentr√©e est un vecteur de taille *embedding_dim* et la sortie vaut 1.\n",
        "\n",
        "Codez cette transformation et v√©rifiez la forme du r√©sultat final."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. On initialise des poids reproductibles avec `manual_seed`.\n",
        "2. On cr√©e une couche lin√©aire pour transformer un vecteur d‚Äôentr√©e en un score.\n",
        "3. On applique Sigmoid pour obtenir une probabilit√©.\n",
        "4. On affiche la sortie et sa forme pour v√©rifier que tout fonctionne."
      ],
      "metadata": {
        "id": "Vw8P4ZijEqwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA4Ekg8ypySW",
        "outputId": "8ac98391-4be8-466d-98ef-8190c74e61d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1]) tensor([0.6601], grad_fn=<SigmoidBackward>)\n",
            "torch.Size([1]) tensor([0.8424], grad_fn=<SigmoidBackward>)\n"
          ]
        }
      ],
      "source": [
        "# Compute out, after you created the Linear layer\n",
        "th.manual_seed(12) # set the seed\n",
        "W  = th.nn.Linear(in_features=D, out_features=1)\n",
        "out_activation = th.nn.Sigmoid()\n",
        "out= out_activation(W(sumOfEmbs))\n",
        "print(out.shape,out)\n",
        "\n",
        "W  = th.nn.Linear(in_features=D, out_features=1)\n",
        "out_activation = th.nn.Sigmoid()\n",
        "out= out_activation(W(sumOfEmbs))\n",
        "print(out.shape,out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cr√©er la couche lin√©aire\n",
        "\n",
        "* `Linear` cr√©e une couche **lin√©aire (ou fully connected)**.\n",
        "* `in_features=D` ‚Üí nombre de **caract√©ristiques d‚Äôentr√©e** (dimension de ton vecteur `sumOfEmbs`).\n",
        "* `out_features=1` ‚Üí sortie unique (souvent utilis√© pour un **score ou probabilit√©**).\n",
        "* Derri√®re le d√©cor : PyTorch cr√©e une **matrice de poids `D x 1`** et un **biais `1`**, tous initialis√©s al√©atoirement.\n",
        "\n",
        "---\n",
        "\n",
        "### Appliquer la fonction d‚Äôactivation Sigmoid\n",
        "\n",
        "```python\n",
        "out_activation = th.nn.Sigmoid()\n",
        "out = out_activation(W(sumOfEmbs))\n",
        "```\n",
        "\n",
        "* `W(sumOfEmbs)` : applique la **couche lin√©aire** √† ton vecteur d‚Äôentr√©e `sumOfEmbs`.\n",
        "* `Sigmoid()` : transforme la sortie en une **valeur comprise entre 0 et 1**, utile pour des **probabilit√©s**.\n",
        "* R√©sultat final : `out` est un **tenseur** avec la forme `(1,1)` si `sumOfEmbs` est un vecteur.\n",
        "\n",
        "---\n",
        "\n",
        "### Affichage du r√©sultat\n",
        "\n",
        "* `out.shape` ‚Üí forme du tenseur (ex: `[1, 1]`).\n",
        "* `out` ‚Üí la valeur calcul√©e par la couche lin√©aire suivie de Sigmoid.\n",
        "* Comme tu recr√©es la couche avec les m√™mes param√®tres, mais apr√®s avoir r√©initialis√©, tu auras **des poids diff√©rents** √† chaque cr√©ation si tu ne fixes pas la graine avant chaque ligne."
      ],
      "metadata": {
        "id": "EnNgkB12EAEk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIOT0Mm3pySW"
      },
      "source": [
        "## Encapsuler le tout dans un module/mod√®le\n",
        "\n",
        "Pour impl√©menter le mod√®le, nous proposons de remplir la classe suivante. Pour √©crire votre propre module, h√©ritez de la classe *Module*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLWaKdIEpySX"
      },
      "outputs": [],
      "source": [
        "class CBOW_classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW_classifier, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lin = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        return th.sigmoid(self.lin(self.emb(inp).sum(dim=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW est une technique issue du traitement du langage naturel (NLP) pour repr√©senter le contexte d‚Äôun mot via des vecteurs.\n",
        "* Ici, on utilise cette repr√©sentation pour pr√©dire une **sortie binaire** √† partir d‚Äôun ensemble de mots.\n",
        "\n",
        "---\n",
        "\n",
        "### Les composants du mod√®le\n",
        "\n",
        "Le mod√®le est une classe `CBOW_classifier` h√©ritant de `nn.Module` (base des mod√®les PyTorch).\n",
        "\n",
        "```python\n",
        "self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "```\n",
        "\n",
        "* Cr√©e une **matrice d‚Äôembeddings** de taille `(vocab_size, embedding_dim)`.\n",
        "* Chaque mot du vocabulaire est repr√©sent√© par un vecteur de `embedding_dim` dimensions.\n",
        "* Exemple : si `vocab_size = 10000` et `embedding_dim = 50`, chaque mot devient un vecteur de 50 nombres.\n",
        "\n",
        "```python\n",
        "self.lin = nn.Linear(embedding_dim, 1)\n",
        "```\n",
        "\n",
        "est une **couche lin√©aire** qui prend le vecteur moyen des mots et produit un score unique.\n",
        "Ce score sera ensuite transform√© en probabilit√© pour la classification binaire.\n",
        "\n",
        "---\n",
        "\n",
        "### Le `forward` (propagation avant)\n",
        "\n",
        "```python\n",
        "def forward(self, inp):\n",
        "    return th.sigmoid(self.lin(self.emb(inp).sum(dim=0)))\n",
        "```\n",
        "\n",
        "* `self.emb(inp)` : transforme les indices des mots (`inp`) en vecteurs d‚Äôembeddings.\n",
        "* `.sum(dim=0)` : **additionne les vecteurs** des mots pour obtenir une repr√©sentation globale du contexte (bag-of-words).\n",
        "* `self.lin(...)` : passe cette repr√©sentation dans la couche lin√©aire pour produire un score.\n",
        "* `th.sigmoid(...)` : transforme ce score en **probabilit√© entre 0 et 1** pour la classification binaire.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualisation du flux\n",
        "\n",
        "```\n",
        "Indices des mots  --> Embeddings --> Somme (CBOW) --> Couche lin√©aire --> Sigmoid --> Probabilit√©\n",
        "```\n",
        "\n",
        "* CBOW simplifie le contexte en **somme des vecteurs de mots**, donc on perd l‚Äôordre des mots.\n",
        "* Embeddings permettent de passer du texte brut √† des vecteurs num√©riques exploitables par les mod√®les ML.\n",
        "* Ce mod√®le est **simple mais efficace** pour de petites t√¢ches de classification de texte.\n",
        "* Adaptable : on peut changer `embedding_dim` ou ajouter des couches suppl√©mentaires pour plus de complexit√©."
      ],
      "metadata": {
        "id": "YZwTAyxgFYFe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cs5wXpWwpySX"
      },
      "source": [
        "Cette classe h√©rite de *Module*. Ces deux m√©thodes sont obligatoires. Le constructeur cr√©e un mod√®le avec ses param√®tres initialis√©s. La m√©thode *forward* sert √† l'inf√©rence. Rappel¬†: en PyTorch, un mod√®le prend des *Tensors (variables)* et renvoie des *Tensors (variables)*. La sortie est compar√©e au mod√®le de r√©f√©rence par la fonction de perte.\n",
        "\n",
        "Imprimez cette classe et effectuez un test simple¬†: prenez un exemple d'entra√Ænement et v√©rifiez si la propagation avant est correcte. Le r√©sultat doit √™tre un *FloatTensor* contenant une seule valeur¬†: le score (entre 0 et 1) attribu√© par le mod√®le √† l'exemple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIlrnlyMpySX",
        "outputId": "9fd6769d-002a-4cad-875e-ffef4b306831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6236], grad_fn=<SigmoidBackward>)\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "classifier = CBOW_classifier(vocab_size=len(lexicon),embedding_dim=10)\n",
        "print(classifier(texts[0]))\n",
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWSldy6pySY",
        "outputId": "bb7b2097-d081-4f63-b030-ffa7884b8395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6236], grad_fn=<SigmoidBackward>)\n"
          ]
        }
      ],
      "source": [
        "print(classifier.forward(texts[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEqE-4ippySY"
      },
      "source": [
        "## Fonction objectif\n",
        "\n",
        "La fonction de perte (ou fonction objectif) est adapt√©e au mod√®le et √† la t√¢che.\n",
        "\n",
        "- Consultez la documentation du module **nn**¬†: http://pytorch.org/docs/master/nn.html.\n",
        "\n",
        "- Dans notre cas, deux fonctions de perte peuvent √™tre utilis√©es¬†: *BCELoss* et *BCEWithLogitsLoss*. Comparez-les et faites votre choix.\n",
        "\n",
        "- En fonction de ce choix, vous pouvez modifier la classe *CBOW_Classifier*."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quel **objet PyTorch** devez-vous utiliser pour calculer la **fonction de perte adapt√©e √† ce probl√®me de classification binaire**, et comment l‚Äôinstancier dans votre code‚ÄØ?\n",
        "\n",
        "* La sortie de votre mod√®le est un **score entre 0 et 1** (apr√®s sigmoid).\n",
        "* La fonction de perte doit comparer la pr√©diction du mod√®le avec la **v√©ritable √©tiquette (0 ou 1)**.\n",
        "* Cherchez dans `torch.nn` une classe correspondant √† ce type de probl√®me."
      ],
      "metadata": {
        "id": "-tbxejm9J6PO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOg8oBhTpySY"
      },
      "outputs": [],
      "source": [
        "## TODO : define de training function\n",
        "loss_fn = nn.BCELoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pDdDsFBmpySZ"
      },
      "source": [
        "## Entra√Ænement\n",
        "\n",
        "√âcrivez le code permettant d'entra√Æner votre mod√®le, de suivre le processus d'entra√Ænement et d'√©valuer le mod√®le √† l'aide des donn√©es de test. Le mod√®le commence avec un optimiseur SGD et un taux d'apprentissage de 0,1.\n",
        "\n",
        "## Ordre al√©atoire\n",
        "\n",
        "Dans de nombreux cas, il est important de parcourir les donn√©es dans un ordre al√©atoire, et non dans l'ordre de construction du corpus. Cet ordre initial peut introduire un biais dans l'√©valuation. Une m√©thode simple pour m√©langer les donn√©es consiste √† m√©langer les indices utilis√©s. Supposons que nous ayons 10 exemples d'entra√Ænement¬†:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous devez g√©n√©rer une liste contenant les nombres de 0 √† 9 inclus.\n",
        "Puis, m√©langez al√©atoirement cette liste pour que l‚Äôordre des √©l√©ments change √† chaque ex√©cution.\n",
        "Enfin, affichez chaque √©l√©ment du nouveau m√©lange, un par un.\n",
        "\n",
        "**Questions √† se poser pour avancer :**\n",
        "\n",
        "1. Comment cr√©er une liste de nombres cons√©cutifs automatiquement ?\n",
        "2. Existe-t-il une m√©thode pour m√©langer les √©l√©ments d‚Äôune liste de fa√ßon al√©atoire en Python ?\n",
        "3. Quelle structure de boucle permet d‚Äôafficher chaque √©l√©ment individuellement ?\n",
        "\n",
        "**Challenge suppl√©mentaire :**\n",
        "\n",
        "* Modifiez le code pour que la liste puisse contenir n‚Äôimporte quel intervalle d‚ÄôIDs fourni par l‚Äôutilisateur.\n",
        "* Affichez la liste finale dans un ordre invers√© apr√®s le m√©lange."
      ],
      "metadata": {
        "id": "Y_FuuT56KRVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yPZ4J8ypySZ",
        "outputId": "207db18a-a6c1-461d-d9f3-508e0e18256d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "0\n",
            "2\n",
            "3\n",
            "8\n",
            "7\n",
            "9\n",
            "6\n",
            "1\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "ids = list(range(10))\n",
        "import random\n",
        "random.shuffle(ids)\n",
        "for i in ids:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1N9HuIpySZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLbvcefpySZ"
      },
      "source": [
        "##### Nous avons maintenant tout ce qu'il faut pour ex√©cuter la boucle d'entra√Ænement et tester ce mod√®le."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FN8eCwkpySa",
        "outputId": "a7be379c-0a06-408e-c669-c3cb0efd7906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor(0.7279) 54.54333333333334 tensor(15049.)\n",
            "1 tensor(0.6946) 58.906666666666666 tensor(15068.)\n",
            "2 tensor(0.6643) 62.86333333333334 tensor(14997.)\n",
            "3 tensor(0.6356) 65.49333333333334 tensor(15064.)\n",
            "4 tensor(0.6110) 67.36 tensor(14950.)\n",
            "5 tensor(0.5916) 68.86666666666666 tensor(14988.)\n",
            "6 tensor(0.5753) 69.91333333333333 tensor(14934.)\n",
            "7 tensor(0.5615) 71.37 tensor(14945.)\n",
            "8 tensor(0.5477) 72.33666666666667 tensor(14995.)\n",
            "9 tensor(0.5371) 72.68 tensor(14916.)\n"
          ]
        }
      ],
      "source": [
        "total = len(texts)\n",
        "randomidx = list(range(total))\n",
        "preds = th.zeros(total)\n",
        "optimizer = th.optim.SGD(classifier.parameters(),lr=1e-2)\n",
        "Nepochs = 10\n",
        "losses = th.zeros(Nepochs)\n",
        "for epoch in range(Nepochs):\n",
        "    total_loss = th.Tensor([0])\n",
        "    correct=0\n",
        "    random.shuffle(randomidx)\n",
        "    for i in randomidx:\n",
        "        classifier.zero_grad()\n",
        "        x = texts[i]\n",
        "        probs = classifier(x)[0]\n",
        "        loss = loss_fn(probs, labels[i])\n",
        "        pred= probs>0.5\n",
        "        preds[i] = pred\n",
        "        if pred.item() == labels[i].item() :\n",
        "            correct +=1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "    losses[epoch] = total_loss/total\n",
        "    print(epoch, losses[epoch], 100.0*correct/total, preds.sum())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code effectue **une boucle d‚Äôentra√Ænement manuelle** (pas de `DataLoader`), ce qui est p√©dagogique mais peu optimis√© pour de grands datasets.\n",
        "* Utilisation de **SGD simple**, sans momentum ni autres techniques modernes d‚Äôoptimisation.\n",
        "* Les pr√©dictions sont binaris√©es avec un **seuil de 0.5**, ce qui est standard pour la classification binaire.\n",
        "* La pr√©cision et la perte sont suivies **√©poque par √©poque** pour monitorer l‚Äôapprentissage.\n",
        "* `total_loss += loss.data` utilise `.data` pour extraire la valeur brute de la loss :\n",
        "\n",
        "1. `total` correspond au nombre total d‚Äôexemples dans le dataset.\n",
        "\n",
        "2. Cr√©ation d‚Äôune liste d‚Äôindices `[0, 1, ..., total-1]` qui servira √† parcourir les exemples de mani√®re al√©atoire.\n",
        "\n",
        "3. Initialisation d‚Äôun vecteur de z√©ros pour stocker les pr√©dictions du mod√®le pour chaque exemple.\n",
        "\n",
        "4. D√©finition de l‚Äôoptimiseur : **Stochastic Gradient Descent** avec un **learning rate** de 0.01.\n",
        "Il est appliqu√© aux param√®tres du mod√®le `classifier`.\n",
        "\n",
        "5. * `Nepochs` d√©finit le nombre de passes compl√®tes sur l‚Äôensemble du dataset.\n",
        "   * `losses` va stocker la perte moyenne pour chaque √©poque.\n",
        "\n",
        "---\n",
        "\n",
        "### Boucle d‚Äôentra√Ænement\n",
        "\n",
        "6. Pour chaque √©poque :\n",
        "\n",
        "     * `total_loss` initialise la somme des pertes.\n",
        "     * `correct` comptera le nombre de pr√©dictions correctes.\n",
        "     * `random.shuffle(randomidx)` m√©lange l‚Äôordre des exemples pour rendre l‚Äôentra√Ænement plus robuste.\n",
        "\n",
        "7. Pour chaque exemple :\n",
        "\n",
        "     * `classifier.zero_grad()` r√©initialise les gradients pour √©viter l‚Äôaccumulation.\n",
        "     * `x` est l‚Äôexemple courant.\n",
        "     * `probs` est la sortie du mod√®le pour cet exemple. Comme il s‚Äôagit d‚Äôun probl√®me binaire, c‚Äôest la probabilit√© d‚Äôappartenance √† la classe positive.\n",
        "\n",
        "8. Calcul de la **loss** (fonction de co√ªt) entre la pr√©diction `probs` et le label r√©el `labels[i]`.\n",
        "   \n",
        "9. * Conversion de la probabilit√© en pr√©diction binaire (1 si >0.5, sinon 0).\n",
        "   * Stockage dans `preds`.\n",
        "   * Incr√©mentation de `correct` si la pr√©diction correspond au label r√©el.\n",
        "\n",
        "10. La perte est accumul√©e pour le calcul de la perte moyenne :\n",
        "- `loss.backward()` calcule les gradients.  \n",
        "- `optimizer.step()` met √† jour les param√®tres du mod√®le.  \n",
        "\n",
        "---\n",
        "\n",
        "Apr√®s chaque √©poque,\n",
        "\n",
        "* Calcul de la **perte moyenne** pour l‚Äô√©poque.\n",
        "* Affichage :\n",
        "\n",
        "  * `epoch` : num√©ro de l‚Äô√©poque.\n",
        "  * `losses[epoch]` : perte moyenne.\n",
        "  * `100*correct/total` : pr√©cision en pourcentage.\n",
        "  * `preds.sum()` : nombre de pr√©dictions positives."
      ],
      "metadata": {
        "id": "MvBO4qXLLoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisez l‚Äô√©volution de la perte au cours du temps pour analyser la convergence du mod√®le.\n",
        ">\n",
        "> **Consignes :**\n",
        ">\n",
        "> 1. Utilisez Matplotlib pour cr√©er le graphique.\n",
        "> 2. L‚Äôaxe des abscisses doit repr√©senter les it√©rations.\n",
        "> 3. L‚Äôaxe des ordonn√©es doit repr√©senter les valeurs de `losses`.\n",
        "> 4. Ajoutez un titre et des labels pour chaque axe.\n",
        "\n",
        "---\n",
        "\n",
        "**Challenge suppl√©mentaire :**\n",
        "\n",
        "* Ajoutez une couleur diff√©rente ou un style de ligne (`'r--'` par exemple).\n",
        "* Affichez les valeurs de perte au-dessus de chaque point du graphique."
      ],
      "metadata": {
        "id": "-JUBNOBQO5Fp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvduf49XpySa",
        "outputId": "1435f946-064f-47ef-c8f8-f1fc951d87ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f57963942d0>]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5d3/8fc3CQHCFgKBQMAEFZBNQcLuBrUKagE3BJTFItSFVn3aPtXnaX+1Vrs8tbXV4oIgioKoaBW1iFatVvaAyBL2PbKFfV9Cvr8/ZmjHGGAkISfJfF7XNZcz97nPme+ZS+aTc8859zF3R0REYk9c0AWIiEgwFAAiIjFKASAiEqMUACIiMUoBICISoxKCLuDbqFu3rmdmZgZdhohIuTJv3rzt7p5auL1cBUBmZibZ2dlBlyEiUq6Y2fqi2jUEJCISoxQAIiIxSgEgIhKjFAAiIjFKASAiEqMUACIiMUoBICISo2IiAP6Rs5XJ83KDLkNEpEyJKgDMrKeZLTezVWb2QBHLHzezBeHHCjPbHW5va2YzzWyJmS00s1si1nnBzNZGrNe25HbrP9ydiXM28NPJX/LWF1+djbcQESmXTnslsJnFA6OA7wK5wFwzm+LuOSf6uPv9Ef1/CLQLvzwIDHb3lWbWEJhnZtPcfXd4+U/dfXIJ7cvJ6mfUwIv5/gtz+a/XFhAfZ3zvooZn8y1FRMqFaI4AOgKr3H2Nux8FJgF9TtF/APAKgLuvcPeV4eebgG3AN+ajONuqJsYzdmgWWRkp3PfqAt5fvLm0SxARKXOiCYB0YGPE69xw2zeYWQbQBPi4iGUdgURgdUTzo+GhocfNrPJJtjnCzLLNLDsvLy+KcouWlJjA87d3oG3jZEZO/IIPc7ae8bZERCqCaALAimg72Y2E+wOT3f341zZg1gB4Cbjd3QvCzQ8CFwAdgBTgZ0Vt0N1Hu3uWu2elphbv4KF65QTG3d6BVum1uHvCPD5Ztq1Y2xMRKc+iCYBcoHHE60bAppP07U94+OcEM6sJvAf83N1nnWh3980ecgQYR2io6ayrWaUS47/fkeZpNfjBy/P4bMWZH1WIiJRn0QTAXKCpmTUxs0RCX/JTCncys+ZAbWBmRFsi8DdgvLu/Xqh/g/B/DegLLD7Tnfi2alWtxMvDOnFeanWGj89mxqrtpfXWIiJlxmkDwN3zgZHANGAp8Jq7LzGzh82sd0TXAcAkd48cHuoHXAYMLeJ0zwlmtghYBNQFHimB/YlaclIiE+7oRGadagx7MZvZa3aU5tuLiATOvv59XbZlZWV5Sd8QJm/fEfqPnsnmPYcZ//2OZGWmlOj2RUSCZmbz3D2rcHtMXAl8Kqk1KvPK8M6k1azC0HFz+WLDrqBLEhEpFTEfAAD1alZh4vDO1KmeyODn57Awd/fpVxIRKecUAGFptUIhUKtqJQaNncPir/YEXZKIyFmlAIiQnlyVV4Z3plpiPIPGzmbZlr1BlyQictYoAAppnJLEKyM6Uzkhnlufm83KrfuCLklE5KxQABQho041Jg7vRFycMeC52azO2x90SSIiJU4BcBLnplbnleGdAGfgc7NYt/1A0CWJiJQoBcApnF+vBhPu6Myx46EQ2LjzYNAliYiUGAXAaTRPq8HLwzpx4Ohx+o+eRe4uhYCIVAwKgCi0bFiTl4d1Yu/hYwx8bjab9xwKuiQRkWJTAESpTaNavDSsE7sOHGXgc7PZuvdw0CWJiBSLAuBbaNs4mRe+34Ftew8z8LlZ5O07EnRJIiJnTAHwLbXPSGHc7R3ZtPswt46ZxY79CgERKZ8UAGegY5MUxg7NYsPOg9w6Zja7DhwNuiQRkW9NAXCGup5Xl+cGZ7Fm+wFuGzubPQePBV2SiMi3ogAohkubpvLsoPas3Lqfwc/PZu9hhYCIlB8KgGLq3rweT916MTmb9zLk+TnsP5IfdEkiIlGJKgDMrKeZLTezVWb2QBHLH4+45eMKM9sdsWyIma0MP4ZEtLc3s0XhbT4RvjdwuXRly/o8OeBiFubu4fZxczigEBCRcuC0AWBm8cAooBfQEhhgZi0j+7j7/e7e1t3bAk8Cb4bXTQF+CXQCOgK/NLPa4dWeBkYATcOPniWyRwHp2TqNJ/q3Y976XQx7cS6Hjh4PuiQRkVOK5gigI7DK3de4+1FgEtDnFP0HAK+En18NfOjuO919F/Ah0NPMGgA13X1m+Cby44G+Z7wXZcS1Fzbg8VvaMmftToaPz+bwMYWAiJRd0QRAOrAx4nVuuO0bzCwDaAJ8fJp108PPo9nmCDPLNrPsvLy8KMoNVp+26fzhpouYvno7P3hpnkJARMqsaAKgqLF5P0nf/sBkdz/xrXeydaPepruPdvcsd89KTU09bbFlwY3tG/G7G9rw6Yo87p4wn6P5BUGXJCLyDdEEQC7QOOJ1I2DTSfr25z/DP6daNzf8PJptlku3dDiHR/q25uNl2xg5cT7HjisERKRsiSYA5gJNzayJmSUS+pKfUriTmTUHagMzI5qnAVeZWe3wj79XAdPcfTOwz8w6h8/+GQy8Xcx9KXNu65zBr3q34oOcrdw3aQH5CgERKUMSTtfB3fPNbCShL/N44Hl3X2JmDwPZ7n4iDAYAk8I/6p5Yd6eZ/ZpQiAA87O47w8/vAl4AqgJTw48KZ0jXTI4dL+CR95YSH2c8fktb4uPK7RmvIlKBWMT3dZmXlZXl2dnZQZdxRp7+52p+//4yel/UkD/2u4hK8boGT0RKh5nNc/eswu2nPQKQknHXFedhBr+buoz9R/IZNfBiqibGB12WiMQw/Rlaiu68/Dx+c30bPlm+jcHPz2bPIc0dJCLBUQCUsoGdzuGJ/u1YsHE3A0brpjIiEhwFQAC+d1FDxgzpwNrtB+j37EzdaF5EAqEACMjlzVJ5+Y6O7Nh/hJuensmqbfuCLklEYowCIEDtM1J49QddyC9wbn5mJgtzd59+JRGREqIACFiLBjWZfGcXqlVOYMDoWcxYvT3okkQkRigAyoDMutWYfGdX0mtXZei4uXywZEvQJYlIDFAAlBFptarw2g+60LJBTe6aMJ/J83JPv5KISDEoAMqQ5KREJtzRiS7n1uEnr3/J2M/XBl2SiFRgCoAyplrlBMYOzaJnqzR+/W4Of/pgOeVpug4RKT8UAGVQ5YR4/jqwHf2yGvHEx6v45ZQlFBQoBESkZGkuoDIqIT6O3994IclJiYz+bA17Dh3jsZs1iZyIlBwFQBlmZjzY6wKSkyrxf+8vZ9/hfJ669WKqVNIkciJSfPpzsowzM+6+4nwevb51aBK5sXPYe1iTyIlI8SkAyolbO2XwRP92zN+wi/7PzmL7fk0iJyLFowAoR0KTyGWxZvt++j2jSeREpHiiCgAz62lmy81slZk9cJI+/cwsx8yWmNnEcFt3M1sQ8ThsZn3Dy14ws7URy9qW3G5VXFc0r8fLwzqRt/8INz+jSeRE5MydNgDMLB4YBfQCWgIDzKxloT5NgQeBbu7eCrgPwN0/cfe27t4W6AEcBD6IWPWnJ5a7+4IS2aMYkJWZwqsjunDsuCaRE5EzF80RQEdglbuvcfejwCSgT6E+w4FR7r4LwN23FbGdm4Cp7q5xixLQsqEmkROR4okmANKBjRGvc8NtkZoBzcxsupnNMrOeRWynP/BKobZHzWyhmT1uZpWLenMzG2Fm2WaWnZeXF0W5sePEJHINkzWJnIh8e9EEgBXRVviy1ASgKXAFMAAYY2bJ/96AWQOgDTAtYp0HgQuADkAK8LOi3tzdR7t7lrtnpaamRlFubDkxiVyL8CRyb2gSORGJUjQBkAs0jnjdCNhURJ+33f2Yu68FlhMKhBP6AX9z93+fwO7umz3kCDCO0FCTnIHa1UKTyHU+N4Ufv/4lz2sSORGJQjQBMBdoamZNzCyR0FDOlEJ93gK6A5hZXUJDQmsilg+g0PBP+KgAMzOgL7D4THZAQqpXTuD5oR3o2SqNh9/N4U8frtAkciJySqcNAHfPB0YSGr5ZCrzm7kvM7GEz6x3uNg3YYWY5wCeEzu7ZAWBmmYSOID4ttOkJZrYIWATUBR4p/u7EthOTyN3cvhFPfLSShzSJnIicgpWnvxKzsrI8Ozs76DLKPHfnN39fynP/Wkvftg35gyaRE4lpZjbP3bMKt2syuArIzPifa1qQnJTIH6YtZ68mkRORIujPwgrKzLin+/n8um94ErnnNYmciHydAqCCG9Q5g7/0b8f89bsYMFqTyInIfygAYkDvixry3JAsVueFJpH7avehoEsSkTJAARAjujevx0vhSeRuenqGJpETEQVALOkQMYncDU/NYPaaHUGXJCIBUgDEmJYNa/K3u7uSWqMyg8bOYcqXhS/qFpFYoQCIQY1Tknjjrq60bZzMj175gmc/Xa2rhkVikAIgRiUnJTJ+WEeuvbABv526jP/39hKO66phkZiiC8FiWJVK8TzZvx2Nkqvy7Gdr2LznEE8MaEdSov63EIkFOgKIcXFxxoPXtODhPq34eNk2BoyeRd4+XSsgEgsUAALA4C6ZPHNbe5Zv3ccNT09ndd7+oEsSkbNMASD/dlWrNF4Z3pmDR45z49MzyF63M+iSROQsUgDI17Q7pzZv3t2V2kmJDBwzm78v2hx0SSJyligA5Bsy6lTjjbu60ia9FvdMnM+Yf63RaaIiFZACQIqUEr7NZM9WaTzy3lJ+9U6OThMVqWAUAHJSVSrFM2rgxQy7pAkvzFjH3RPmcejo8aDLEpESElUAmFlPM1tuZqvM7IGT9OlnZjlmtsTMJka0HzezBeHHlIj2JmY228xWmtmr4fsNSxkTF2f84rqW/L/rWvJBzlYGjpnFDk0pLVIhnDYAzCweGAX0AloCA8ysZaE+TYEHgW7u3gq4L2LxIXdvG370jmj/PfC4uzcFdgHDircrcjZ9/5ImPH3rxeRs2suNT89g3fYDQZckIsUUzRFAR2CVu69x96PAJKBPoT7DgVHuvgvA3bedaoNmZkAPYHK46UWg77cpXEpfz9YNmDi8E3sOHeOGp2cwf8OuoEsSkWKIJgDSgY0Rr3PDbZGaAc3MbLqZzTKznhHLqphZdrj9xJd8HWC3u+efYpsAmNmI8PrZeXl5UZQrZ1P7jBTevLsbNaokMGD0LN5fvCXokkTkDEUTAFZEW+HTQRKApsAVwABgjJklh5edE74b/UDgz2Z2XpTbDDW6j3b3LHfPSk1NjaJcOdua1A2dJtqiQU3umjCPF6avDbokETkD0QRALtA44nUjoPAk8rnA2+5+zN3XAssJBQLuvin83zXAP4F2wHYg2cwSTrFNKcPqVq/MK8M7c2WL+jz0Tg6PvJtDgU4TFSlXogmAuUDT8Fk7iUB/YEqhPm8B3QHMrC6hIaE1ZlbbzCpHtHcDcjx0VdEnwE3h9YcAbxd3Z6R0VU2M55nb2jOkSwZjPl/LyFfmc/iYThMVKS9OGwDhcfqRwDRgKfCauy8xs4fN7MRZPdOAHWaWQ+iL/afuvgNoAWSb2Zfh9t+5e054nZ8B/2Vmqwj9JjC2JHdMSkd8nPFQ71b87zUt+PuiLdw2Zja7DhwNuiwRiYKVp0v8s7KyPDs7O+gy5CTeW7iZ+19bQKPkqrxwe0fOqZMUdEkiApjZvPBvsV+jK4GlxFx7YQMm3NGJnQePcv1T01mwcXfQJYnIKSgApER1yEzhjbu6klQ5nv6jZ/JhztagSxKRk1AASIk7L7U6b97VjWb1a/CDl7J5aea6oEsSkSIoAOSsSK1RmUkjOtO9eT1+8fYSfjt1qU4TFSljFABy1iQlJvDsoPbc1vkcnv10Dfe+uoAj+TpNVKSsSDh9F5EzlxAfx6/7tCY9OYnfv7+MrXsP89ygLGolVQq6NJGYpyMAOevMjLuuOI+/9G/Lgg27ufGZGWzceTDoskRingJASk2ftumMH9aRbXsPc8PTM1iUuyfokkRimgJASlXnc+vwxl1dSYyP45bRM3nnS00BJRIUBYCUuqb1a/C3u7tyQVoNfvjKF/zircX6cVgkAAoACUS9mlV49QddGH5pE16atZ6bnp7Jhh36XUCkNCkAJDCV4uP432tbMnpQe9bvOMC1T/5LN5gRKUUKAAncVa3SeO9Hl9KkbjXufHkev343h6P5BUGXJVLhKQCkTGicksTrd3ZhaNdMxn6+ln7PzuSr3YeCLkukQlMASJlROSGeh3q3YtTAi1m1bT/XPvEvPl6myeREzhYFgJQ5117YgHd+eAkNa1Xl+y9k87upy8g/riEhkZKmAJAyqUndarx5d1cGdDyHZz5dzcDnZrNlz+GgyxKpUKIKADPraWbLzWyVmT1wkj79zCzHzJaY2cRwW1szmxluW2hmt0T0f8HM1prZgvCjbcnsklQUVSrF89sb2vDnW9qyeNMern3iX3y2Ii/oskQqjNMGgJnFA6OAXkBLYICZtSzUpynwINDN3VsB94UXHQQGh9t6An82s+SIVX/q7m3DjwXF3x2piPq2S2fKyEuoUz2RIePm8KcPV3BcU0uLFFs0RwAdgVXuvsbdjwKTgD6F+gwHRrn7LgB33xb+7wp3Xxl+vgnYBqSWVPESO86vV5237unGjRc34omPVjJo7Gy27dOQkEhxRBMA6cDGiNe54bZIzYBmZjbdzGaZWc/CGzGzjkAisDqi+dHw0NDjZla5qDc3sxFmlm1m2Xl5OvyPZUmJCTx280X8300XMn/DLq594nNmrt4RdFki5VY0AWBFtBU+/k4AmgJXAAOAMZFDPWbWAHgJuN3dT5zO8SBwAdABSAF+VtSbu/tod89y96zUVB08CPTLasxb93SjRpUEbh0zi79+vFJ3GxM5A9EEQC7QOOJ1I6DwFI65wNvufszd1wLLCQUCZlYTeA/4ubvPOrGCu2/2kCPAOEJDTSJRuSCtJlNGXsJ1FzbksQ9WMPSFuew8cDToskTKlWgCYC7Q1MyamFki0B+YUqjPW0B3ADOrS2hIaE24/9+A8e7+euQK4aMCzMyAvsDi4uyIxJ7qlRP4S/+2PHp9a2at2cE1f/kX2et2Bl2WSLlx2gBw93xgJDANWAq85u5LzOxhM+sd7jYN2GFmOcAnhM7u2QH0Ay4DhhZxuucEM1sELALqAo+U6J5JTDAzbu2UwZt3daVypThuGT2LZz9drSEhkSiYe/n5h5KVleXZ2dlBlyFl1N7Dx/jZ5IVMXbyFK1vU47GbLyI5KTHoskQCZ2bz3D2rcLuuBJYKo2aVSjx168U89L2WfLoij2uf+JwvNuwKuiyRMksBIBWKmTG0WxNev7MrAP2encm46WspT0e6IqVFASAVUtvGybz3o0u4vFkqv3onh7snzGfv4WNBlyVSpigApMJKTkrkucFZ/O81LfggZyvXPfE5i7/aE3RZImWGAkAqNDNj+GXn8uqIzhzNL+CGp2bw8qz1GhISQQEgMSIrM4W/33spXc6rw8/fWsy9kxaw/0h+0GWJBEoBIDEjpVoi44Z24KdXN+fdhZvo/eTnLNuyN+iyRAKjAJCYEhdn3NP9fCbc0Zl9R/Lp89fpTJqzQUNCEpMUABKTupxXh7//6FLaZ9TmgTcX0X/0LB0NSMxRAEjMSq1RmZeGdeLR61uzfOs+rn3icx6asoQ9h3S6qMQGBYDEtPi40FxCn/z4CgZ0bMyLM9fR47F/8lr2Rs0nJBWeAkAEqF0tkUf6tuGdkZeQUSeJ/568kBuensHC3N1BlyZy1igARCK0Tq/F5Du78sebLyJ31yH6jJrOg28u1L0GpEJSAIgUEhdn3Ni+ER//5HKGdWvCa9m5dH/sn7w0c51uRi8VigJA5CRqVqnEz69rydR7L6VVw5r84u0lfO/Jz5mrm85IBaEAEDmNZvVrMOGOTjx168XsPniUm5+Zyf2vLmDb3sNBlyZSLAoAkSiYGde0acA/fnw5I7ufz3sLN9Pjj5/y3GdrOHa8IOjyRM5IVAFgZj3NbLmZrTKzB07Sp5+Z5ZjZEjObGNE+xMxWhh9DItrbm9mi8DafCN8bWKRMS0pM4CdXN+eD+y+jY5MUHv37Unr95V98vnJ70KWJfGunDQAziwdGAb2AlsAAM2tZqE9T4EGgm7u3Au4Lt6cAvwQ6AR2BX5pZ7fBqTwMjgKbhR8+S2CGR0pBZtxrPD+3A2CFZHDtewG1jZ3PXy/PI3XUw6NJEohbNEUBHYJW7r3H3o8AkoE+hPsOBUe6+C8Ddt4XbrwY+dPed4WUfAj3NrAFQ091nemgSlvFA3xLYH5FS9Z0W9Zl232X85KpmfLJ8G1f+6VOe/Gglh48dD7o0kdOKJgDSgY0Rr3PDbZGaAc3MbLqZzTKznqdZNz38/FTbBMDMRphZtpll5+XlRVGuSOmqUimekT2a8tGPr6DHBfX444cruOrxz/ho6dagSxM5pWgCoKix+cInQycQGsa5AhgAjDGz5FOsG802Q43uo909y92zUlNToyhXJBjpyVV56tb2TLijE4kJcQx7MZvbx81h7fYDQZcmUqRoAiAXaBzxuhGwqYg+b7v7MXdfCywnFAgnWzc3/PxU2xQpl7qdX5ep917Kz69twdx1u7j68c/4w7RlHDyqG9BI2RJNAMwFmppZEzNLBPoDUwr1eQvoDmBmdQkNCa0BpgFXmVnt8I+/VwHT3H0zsM/MOofP/hkMvF0ieyRSBlSKj+OOS8/l4x9fznUXNWDUJ6v5zh8/5b2Fm3XvASkzThsA7p4PjCT0Zb4UeM3dl5jZw2bWO9xtGrDDzHKAT4CfuvsOd98J/JpQiMwFHg63AdwFjAFWAauBqSW4XyJlQr2aVfhTv7ZMvrMLtZMSuWfifAY+N5sVW/cFXZoIVp7+GsnKyvLs7OygyxA5I8cLnIlzNvDYtOXsP5LP0K6Z3HtlU2pWqRR0aVLBmdk8d88q3K4rgUVKSXycMahzBp/85Ar6ZTXm+elr6fHYp0yel6t7D0ggFAAipSylWiK/vaENU+65hMYpVfnJ619y0zO694CUPgWASEDaNKrFG3d25Q83XciGnQfp/dfpjBifzeKv9gRdmsSIhKALEIllcXHGzVmNubp1GuM+X8eYz9fwQc5WrmpZnx99pymt02sFXaJUYPoRWKQM2XPoGC9MDwXBvsP5CgIpESf7EVgBIFIGKQikJCkARMohBYGUBAWASDmmIJDiUACIVAAKAjkTCgCRCkRBIN+GAkCkAlIQSDQUACIVmIJATkUBIBIDFARSFAWASAxREEgkBYBIDFIQCCgARGKagiC2KQBEREEQo4p1Qxgz62lmy81slZk9UMTyoWaWZ2YLwo87wu3dI9oWmNlhM+sbXvaCma2NWNa2uDspIqdWq2ol7r2yKZ//rAf3X9mMmWt2cN2Tn2sa6hh12iMAM4sHVgDfBXIJ3dt3gLvnRPQZCmS5+8hTbCeF0P1/G7n7QTN7AXjX3SdHW6yOAERKVlFHBPd0P5+LGicHXZqUoJMdAURzP4COwCp3XxPe0CSgD5BzyrW+6SZgqrsf/JbrichZcuKIYGi3zH8HwQc5W2nbOJkhXTO4pk0DKifEB12mnCXRDAGlAxsjXueG2wq70cwWmtlkM2tcxPL+wCuF2h4Nr/O4mVUu6s3NbISZZZtZdl5eXhTlisi3dSIIZjzQg4e+15K9h45x/6tf0vW3H/PYtOVs2n0o6BLlLIhmCOhm4Gp3PzGuPwjo6O4/jOhTB9jv7kfM7E6gn7v3iFjeAFgINHT3YxFtW4BEYDSw2t0fPlUtGgISKR0FBc701dt5ccZ6Plq2lTgzrmpZn8FdMul8bgpmFnSJ8i0UZwgoF4j8i74RsCmyg7vviHj5HPD7QtvoB/ztxJd/eJ3N4adHzGwc8JMoahGRUhAXZ1zaNJVLm6aycedBXp69nlfnbmTq4i00q1+dwV0yub5dOtUq666y5Vk0Q0BzgaZm1sTMEgkN5UyJ7BD+a/6E3sDSQtsYQKHhnxPrWOhPib7A4m9XuoiUhsYpSTzYqwWzHvwO/3fThVSKj+Pnby2m828+4lfvLGFN3v6gS5QzdNr4dvd8MxsJTAPigefdfYmZPQxku/sU4Edm1hvIB3YCQ0+sb2aZhI4gPi206QlmlgoYsAC4s9h7IyJnTZVK8fTLaszN7Rsxf8Nuxs9cx8uz1jNu+joua5bKkC4ZXNG8HvFxGh4qL3QhmIicsW37DjNpzkYmzF7P1r1HaJxSlUGdM+iX1ZjkpMSgy5MwXQksImfNseMFfLBkKy/OXMectTupnBBH37bpDOqSoauMywAFgIiUiqWb9zJ+5nre+uIrDh07TlZGbQZ3zaRnqzQSE6KafEBKmAJARErVnoPHeH3eRl6atZ71Ow6SWqMyAzuew8BO51C/ZpWgy4spCgARCURBgfPpyjzGz1jHJ8vzSIgzerZOY0jXTLIyauuaglJQnOsARETOWFyc0b15Pbo3r8e67Qd4edZ6XsveyLsLN9OiQU2GdMmgT9t0qiZqyonSpiMAESl1B4/m8/aCTbw4Yx3LtuyjZpUEbunQmNs6Z5BRp1rQ5VU4GgISkTLH3Zm7bhcvzlzH+4u3UOBO9+b1GNwlg8uaphKnawpKhIaARKTMMTM6NkmhY5MUtuw5zMQ5G5g4ewNDx80ls04Sg7pkcnNWI2pWqRR0qRWSjgBEpEw5ml/A1MWbeXHGOuZv2E1SYjzXt0tncJdMmqfVCLq8cklDQCJS7izK3cP4met4+8tNHM0voMu5dRjSNYMrW9QnIV7XFERLASAi5dbOA0d5de5GXp61nq92H6JhrSrc2jmD/h0aU6d6kbcSkQgKABEp9/KPF/DRsm2Mn7mO6at2kJgQx/cubMiQrhlc2Ei3sTwZBYCIVCgrt+5j/Mz1vDE/l4NHj9O2cTJDu2bSq02abmNZiAJARCqkvYeP8ca8XMbPXM/a7QeoWz0xPOVEBmm1NOUEKABEpIIrKHA+X7WdF2es4+Pl24g34+rWaQzpkkmHzNieckLXAYhIhRYXZ1zWLJXLmqWyYcd/bmP53sLNXJBWgyFdM+mrKSe+JqrzqMysp5ktN7NVZvZAEcuHmlmemS0IP+6IWHY8on1KRHsTM5ttZivN7NXw7SZFRIrtnDpJ/M81odtY/pr1+CoAAAc/SURBVO6GNgA8+OYiOv3mHzz6Xg4bdhwMuMKy4bRDQGYWD6wAvkvoBvFzgQHunhPRZyiQ5e4ji1h/v7tXL6L9NeBNd59kZs8AX7r706eqRUNAInImippyokfzegzpmskl59et8FNOFGcIqCOwyt3XhDc0CegD5JxyrVMXY0APYGC46UXgIeCUASAicia+MeXE7PVMnLOBwc/P4dy61RjUJYMb28felBPRDAGlAxsjXueG2wq70cwWmtlkM2sc0V7FzLLNbJaZ9Q231QF2u3v+abYpIlKi0mpV4b+uas70B3rw51vaUiupEr96J4cuv/mIX7y1mJVb9wVdYqmJ5gigqGOjwuNG7wCvuPsRM7uT0F/0PcLLznH3TWZ2LvCxmS0C9kaxzdCbm40ARgCcc845UZQrInJ6lRPi6dsunb7t0lmYu5sXZ6zn1ezQHcy6nleHIV0zubJFfeIr8PBQNL8BdAEecverw68fBHD3356kfzyw092/cSdoM3sBeBd4A8gD0tw9v/B7nIx+AxCRs2nH/iNMmruRCbPWs2nPYdKTq/K9ixrSq3UaFzaqVW5PJT3j6wDMLIHQj8DfAb4i9CPwQHdfEtGngbtvDj+/HviZu3c2s9rAwfCRQV1gJtDH3XPM7HXgjYgfgRe6+1OnqkUBICKlIf94Af9Yuo2JczYwY9V28guc9OSqXN0qjV5t0rj4nNrl6sigWBeCmdk1wJ+BeOB5d3/UzB4Gst19ipn9FugN5AM7gbvcfZmZdQWeBQoI/d7wZ3cfG97mucAkIAX4ArjN3Y+cqg4FgIiUtt0Hj/KPpdt4f/FmPlu5naP5BaTWqMxVLevTq3UDOp+bUuZnJtWVwCIixbT/SD4fLwuFwSfL8jh07DjJSZX4bov69GqTRrfz65bJeYgUACIiJejQ0eN8uiKPaUu28I+lW9l3OJ8alRPo0aIevVqncXmzemXmqmNNBSEiUoKqJsbTs3UaPVuncTS/gOmrt/P+oi18kLOFtxdsomqleK5onkrP1mn0uKAeNcrgNQY6AhARKUH5xwuYs3YnUxdvYdqSLWzbd4TE+DguaVqXnq3T+G6L+tSuVroz32gISESklBUUOPM37OL9xVuYungLX+0+RHyc0eXcOvRsncZVrepTr8bZn7JaASAiEiB3Z/FXe5m6eDPvL97Cmu0HMIMOGSlcHR5KSk+uelbeWwEgIlJGuDsrtu7/dxgs2xKafuKiRrXo2boBvVqnkVm3Wom9nwJARKSMWrv9wL/DYGHuHgAuSKtBr9YN6NUmjab1qhfrKmQFgIhIOfDV7kO8v3gL7y/eTPb6XbjDuanVeOa29jSrX+OMtqnTQEVEyoH05KoMu6QJwy5pwra9h5mWs5V/5Gw9K78PKABERMqoejWrMKhzBoM6Z5yV7ZftCSxEROSsUQCIiMQoBYCISIxSAIiIxCgFgIhIjFIAiIjEKAWAiEiMUgCIiMSocjUVhJnlAevPcPW6wPYSLKe80+fxH/osvk6fx9dVhM8jw91TCzeWqwAoDjPLLmoujFilz+M/9Fl8nT6Pr6vIn4eGgEREYpQCQEQkRsVSAIwOuoAyRp/Hf+iz+Dp9Hl9XYT+PmPkNQEREvi6WjgBERCSCAkBEJEbFRACYWU8zW25mq8zsgaDrCYqZNTazT8xsqZktMbN7g66pLDCzeDP7wszeDbqWoJlZsplNNrNl4f9PugRdU1DM7P7wv5PFZvaKmVUJuqaSVuEDwMzigVFAL6AlMMDMWgZbVWDygR+7ewugM3BPDH8Wke4FlgZdRBnxF+B9d78AuIgY/VzMLB34EZDl7q2BeKB/sFWVvAofAEBHYJW7r3H3o8AkoE/ANQXC3Te7+/zw832E/nGnB1tVsMysEXAtMCboWoJmZjWBy4CxAO5+1N13B1tVoBKAqmaWACQBmwKup8TFQgCkAxsjXucS4196AGaWCbQDZgdbSeD+DPw3UBB0IWXAuUAeMC48JDbGzKoFXVQQ3P0r4DFgA7AZ2OPuHwRbVcmLhQCwItpi+txXM6sOvAHc5+57g64nKGZ2HbDN3ecFXUsZkQBcDDzt7u2AA0BM/mZmZrUJjRQ0ARoC1czstmCrKnmxEAC5QOOI142ogIdy0TKzSoS+/Ce4+5tB1xOwbkBvM1tHaGiwh5m9HGxJgcoFct39xFHhZEKBEIuuBNa6e567HwPeBLoGXFOJi4UAmAs0NbMmZpZI6IecKQHXFAgzM0Lju0vd/U9B1xM0d3/Q3Ru5eyah/y8+dvcK91detNx9C7DRzJqHm74D5ARYUpA2AJ3NLCn87+Y7VMAfxBOCLuBsc/d8MxsJTCP0S/7z7r4k4LKC0g0YBCwyswXhtv9x978HWJOULT8EJoT/WFoD3B5wPYFw99lmNhmYT+jsuS+ogFNCaCoIEZEYFQtDQCIiUgQFgIhIjFIAiIjEKAWAiEiMUgCIiMQoBYCISIxSAIiIxKj/Dw9ZMbr9xfiTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lejz2by5pySa"
      },
      "source": [
        "# Premi√®res exp√©riences (pour aller plus loin en datascience)\n",
        "\n",
        "- Commencez par 2¬†000 exemples, puis utilisez toutes les donn√©es d'entra√Ænement (r√©parties √©quitablement entre exemples positifs et n√©gatifs).\n",
        "\n",
        "- Cr√©ez un ensemble de d√©veloppement et un ensemble de test.\n",
        "\n",
        "- Testez diff√©rentes param√©trisations du mod√®le (ici, la taille de l'embedding) et l'hyperparam√®tre (le taux d'apprentissage) pour chaque configuration.\n",
        "\n",
        "- Comparez ces diff√©rentes configurations (fonction de perte sur l'ensemble d'entra√Ænement et pr√©cision de la classification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdNAgkrLpySa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WkBH35wpySa"
      },
      "source": [
        "# Un mod√®le plus profond\n",
        "\n",
        "On peut ajouter une couche cach√©e au classificateur pr√©c√©dent.\n",
        "\n",
        "- Proc√©der de la m√™me mani√®re qu'auparavant avec les diff√©rentes configurations.\n",
        "\n",
        "- Trouver les hyperparam√®tres optimaux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf5frvuhpySb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}