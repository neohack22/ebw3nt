{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/ebw3nt/blob/main/modelisation/tokenisation_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données brutes de l'IMDB (budgets, nombres de votes, critiques textuelles) sont souvent hétérogènes. Elles ne peuvent pas être injectées directement dans une IA sans une phase de structuration préalable.\n",
        "\n",
        "Cette métamorphose de la donnée suit une séquence logique et rigoureuse au sein du notebook pour garantir la fiabilité du modèle final.\n",
        "\n",
        "## Flux de Travail\n",
        "\n",
        "L'objectif est de convertir un script brut en un \"dataset\" prêt pour l'apprentissage automatique.\n",
        "\n",
        "1. Nettoyage : Suppression des bruits, comme les caractères spéciaux dans les résumés de films ou l'élimination des entrées dont les informations essentielles (comme le titre) sont absentes.\n",
        "2. Normalisation : Mise en conformité des échelles.\n",
        "\n",
        "Exemple : Un film peut avoir 2 000 000 de votes alors que sa note est de 8.5/10. Sans normalisation (ramener ces valeurs sur une échelle de 0 à 1), le modèle d'IA pourrait croire que le nombre de votes est 235 000 fois plus important que la note, ce qui fausserait totalement ses prédictions.\n",
        "\n",
        "3. Structuration : Organisation finale des données en tableaux optimisés, permettant à l'IA de détecter efficacement les corrélations entre le genre d'un film et son succès critique.\n",
        "\n",
        "<br>\n",
        "\n",
        "* Qualité des prédictions : Une IA ne \"comprend\" pas le cinéma ; elle comprend les chiffres. Des données propres garantissent des résultats précis.\n",
        "* Performance du modèle : Des données normalisées et bien structurées permettent un entraînement beaucoup plus rapide et efficace."
      ],
      "metadata": {
        "id": "PTtyTg39DMni"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dca002d"
      },
      "source": [
        "L'objectif est d'implémenter le modèle proposé par Yoon Kim, publié en 2014. L'article original peut être trouvé [ici](https://www.aclweb.org/anthology/D14-1181).\n",
        "Bien sûr, il existe des implémentations PyTorch et TensorFlow sur le web. Elles sont plus ou moins correctes et efficaces.\n",
        "\n",
        "\n",
        "Nous utilisons le même jeu de données qu'auparavant : chargement des données, construction du vocabulaire et préparation des données pour le modèle.\n",
        "\n",
        "\n",
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhmmsZLuc19P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "th.manual_seed(1) # set the seed\n",
        "\n",
        "\n",
        "def clean_str(string, tolower=True):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" \", string) ## remove\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string) ## remove\n",
        "    string = re.sub(r\"\\)\", \" \", string)## remove\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    if tolower:\n",
        "        string = string.lower()\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "def loadTexts(filename, limit=-1):\n",
        "    \"\"\"\n",
        "    Texts loader for imdb.\n",
        "    If limit is set to -1, the whole dataset is loaded, otherwise limit is the number of lines\n",
        "    \"\"\"\n",
        "    f = open(filename)\n",
        "    dataset=[]\n",
        "    line =  f.readline()\n",
        "    cpt=1\n",
        "    skip=0\n",
        "    while line :\n",
        "        cleanline = clean_str(line).split()\n",
        "        if cleanline:\n",
        "            dataset.append(cleanline)\n",
        "        else:\n",
        "            line = f.readline()\n",
        "            skip+=1\n",
        "            continue\n",
        "        if limit > 0 and cpt >= limit:\n",
        "            break\n",
        "        line = f.readline()\n",
        "        cpt+=1\n",
        "\n",
        "    f.close()\n",
        "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_opL6bvMJuf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "th.manual_seed(1) # set the seed\n",
        "\n",
        "\n",
        "def clean_str(string, tolower=True):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" \", string) ## remove\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string) ## remove\n",
        "    string = re.sub(r\"\\)\", \" \", string)## remove\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    if tolower:\n",
        "        string = string.lower()\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "def loadTexts(filename, limit=-1):\n",
        "    \"\"\"\n",
        "    Texts loader for imdb.\n",
        "    If limit is set to -1, the whole dataset is loaded, otherwise limit is the number of lines\n",
        "    \"\"\"\n",
        "    f = open(filename)\n",
        "    dataset=[]\n",
        "    line =  f.readline()\n",
        "    cpt=1\n",
        "    skip=0\n",
        "    while line :\n",
        "        cleanline = clean_str(line).split()\n",
        "        if cleanline:\n",
        "            dataset.append(cleanline)\n",
        "        else:\n",
        "            line = f.readline()\n",
        "            skip+=1\n",
        "            continue\n",
        "        if limit > 0 and cpt >= limit:\n",
        "            break\n",
        "        line = f.readline()\n",
        "        cpt+=1\n",
        "\n",
        "    f.close()\n",
        "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La plupart des algorithmes de traitement du langage naturel s'attendent à ce qu'un seul espace sépare les mots, et des espaces supplémentaires pourraient être interprétés à tort ou simplement causer des problèmes d'analyse.\n",
        "\n",
        "Imaginez que le mot \"Film\" et le mot \"film\" soient traités comme deux mots différents par votre IA. Cela doublerait inutilement le vocabulaire et rendrait plus difficile pour l'IA d'apprendre que ces deux mots ont la même signification. En convertissant tout en minuscules, on s'assure que \"Film\", \"FILM\", et \"film\" sont tous traités comme un seul et même mot : \"film\". C'est une étape fondamentale pour réduire la complexité du vocabulaire et améliorer la performance des modèles de traitement du langage naturel."
      ],
      "metadata": {
        "id": "y3D99y8LOJrG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxGJXdCtc19R"
      },
      "source": [
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfRYf5Qbc19S",
        "outputId": "38f28c2b-448b-4f44-ffa7-70efa3dc20f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load  299966  lines from  /home/allauzen/cours/nlp-iasd/labs/imdb.pos  /  35  lines discarded\n",
            "[['excellent'], ['do', \"n't\", 'miss', 'it', 'if', 'you', 'can'], ['a', 'great', 'parody'], ['dreams', 'of', 'a', 'young', 'girl'], ['tromendous', 'piece', 'of', 'art'], ['funny', 'funny', 'movie', '!'], ['need', 'more', 'scifi', 'like', 'this'], ['pride', 'and', 'prejudice', 'is', 'absolutely', 'amazing', '!', '!'], ['scott', 'pilgrim', 'vs', 'the', 'world'], ['quirky', 'and', 'effective']]\n",
            "299965  pos sentences\n",
            "Load  299949  lines from  /home/allauzen/cours/nlp-iasd/labs/imdb.neg  /  52  lines discarded\n",
            "[['typical', 'movie', 'where', 'best', 'parts', 'are', 'in', 'the', 'preview'], ['not', 'for', 'the', 'squeamish'], ['cool', 'when', 'i', 'was', 'kid'], ['i', 'appreciate', 'the', 'effort', 'but'], ['pretty', 'bad'], ['much', 'ado', 'about', 'nothing'], ['series', 'of', 'unlikely', 'events'], ['april', 'is', 'the', 'cruelest', 'month'], ['great', 'idea', 'but'], ['and', 'people', 'thought', 'this', 'was', 'good', '\\\\?']]\n",
            "299948  neg sentences\n"
          ]
        }
      ],
      "source": [
        "LIM=-1\n",
        "pathd = \"/home/allauzen/cours/nlp-iasd/labs/\"\n",
        "txtfile=pathd+\"imdb.pos\"\n",
        "postxt = loadTexts(txtfile,limit=LIM)\n",
        "print(postxt[0:10])\n",
        "print (len(postxt), \" pos sentences\")\n",
        "\n",
        "txtfile=pathd+\"imdb.neg\"\n",
        "negtxt = loadTexts(txtfile,limit=LIM)\n",
        "print(negtxt[0:10])\n",
        "\n",
        "print (len(negtxt), \" neg sentences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoZvtYM8c19T"
      },
      "outputs": [],
      "source": [
        "wfreq = {}\n",
        "maxlength = 0\n",
        "for sent in postxt+negtxt:\n",
        "    isent = []\n",
        "    maxlength = max(maxlength,len(sent))\n",
        "    for w in sent:\n",
        "        if w in wfreq:\n",
        "            wfreq[w] = wfreq[w]+1\n",
        "        else :\n",
        "            wfreq[w]=1 # On ajoute le mot à notre dictionnaire wfreq et on lui donne un compte de 1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les modèles d'IA ne peuvent pas traiter des phrases de longueurs arbitraires (une phrase de 5 mots, puis une de 50 mots, puis une de 12 mots) de manière directe. Ils ont besoin que toutes les séquences d'entrée aient la même longueur."
      ],
      "metadata": {
        "id": "m2FlKucoYRs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtKDn5Ukc19T",
        "outputId": "60a0d54b-658c-411d-ca0f-d1ce4b817734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63699\n"
          ]
        }
      ],
      "source": [
        "print(len(wfreq)) # nombre total de mots uniques\n",
        "orderedvocab = []\n",
        "for w in sorted(wfreq, key=wfreq.get, reverse=True):\n",
        "    orderedvocab.append((w, wfreq[w]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3NskOyErYfXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDwnOPXKc19U",
        "outputId": "561d61e1-1fc2-4f21-cbcd-15851d5fb5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('!', 153714), ('the', 146409), ('a', 131821), ('of', 94543), ('movie', 80115), ('and', 63910), ('this', 53299), ('to', 46991), ('it', 46431), ('i', 44902)]\n"
          ]
        }
      ],
      "source": [
        "print(orderedvocab[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code transforme notre liste de mots triés en un système de traduction bidirectionnel (mot <-> index) pour que l'IA puisse travailler avec des nombres. Il inclut aussi des \"jokers\" pour gérer les longueurs de phrases et les mots rares."
      ],
      "metadata": {
        "id": "fok8gLEUds1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU3HhMi-c19U",
        "outputId": "e1a6d1c6-1db9-480c-830c-7a55c3dce434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10002  ==  10002\n",
            "1 <unk> 1\n",
            "2 ! 2\n",
            "3 the 3\n",
            "4 a 4\n",
            "5 of 5\n"
          ]
        }
      ],
      "source": [
        "VOCSIZE = 10000\n",
        "w2idx = {}\n",
        "idx2w = {}\n",
        "w2idx[\"<pad>\"]  = 0\n",
        "w2idx[\"<unk>\"] = 1\n",
        "idx2w[1]=\"<unk>\"\n",
        "idx2w[0]=\"<pad>\"\n",
        "\n",
        "for i in range(VOCSIZE):\n",
        "    w, _ = orderedvocab[i]\n",
        "    w2idx[w] = i+2 # 0 et 1 sont déjà pris par <pad> et <unk>\n",
        "    idx2w[i+2] = w\n",
        "\n",
        "print(len(w2idx), \" == \",len(idx2w))\n",
        "for i in range(1,6):\n",
        "    print(i, idx2w[i], w2idx[idx2w[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<pad>` (pour padding) est un mot spécial. Nous en avons parlé quand nous avons discuté de \"dimensionner les entrées\". Il sera utilisé pour remplir (ou \"padder\") les phrases plus courtes afin qu'elles aient toutes la même longueur maximale. On lui attribue généralement l'index 0.\n",
        "\n",
        "`<unk>` (pour unknown) est un autre mot spécial. Il représente tous les mots qui ne sont pas inclus dans notre VOCSIZE de 10 000 mots les plus fréquents, ou que nous n'avons jamais rencontrés pendant la phase d'apprentissage. Tous ces mots \"inconnus\" seront traités comme le même <unk>` par le modèle. On lui attribue l'index 1."
      ],
      "metadata": {
        "id": "3j3tdzmccUCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p9n43DWc19V",
        "outputId": "36d28dc5-47de-4691-a17d-b9d934159b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10002  words in the vocab\n",
            "200000  sentences\n",
            "48  is maximum sentence length\n",
            "tensor([ 36,  25, 381,  10,  58,  21,  83])\n"
          ]
        }
      ],
      "source": [
        "NB_SENTENCES = 100000 # for each class\n",
        "txtidx = []\n",
        "maxlength = 0\n",
        "for sent in postxt[\n",
        "    1:NB_SENTENCES+1]+negtxt[ # la première est généralement utilisée pour des tests ou peut être vide/spéciale\n",
        "        :NB_SENTENCES]:\n",
        "    maxlength = max( # met à jour maxlength si la phrase actuelle est plus longue que toutes celles vues jusqu'à présent\n",
        "        maxlength,len(\n",
        "            sent))\n",
        "    isent=[]\n",
        "    for w in sent:\n",
        "        widx=1 # si un mot n'est pas dans notre vocabulaire w2idx (les 10 000 mots les plus fréquents), il sera traité comme un mot inconnu.\n",
        "        if w in w2idx:\n",
        "            widx=w2idx[w]\n",
        "        isent.append(widx)\n",
        "    txtidx.append(\n",
        "        th.LongTensor( # Une fois que tous les mots de la phrase sent ont été convertis en leurs index numériques dans isent, cette liste est transformée en un torch.LongTensor, type de tenseur PyTorch qui est optimisé pour stocker des entiers\n",
        "            (isent)))\n",
        "\n",
        "print(\n",
        "    len(\n",
        "        w2idx), \" words in the vocab\") # taille de notre vocabulaire final (avec <pad> et <unk>)\n",
        "print(\n",
        "    len(\n",
        "        txtidx), \" sentences\") # 100 000 exemples pour chaque catégorie (positif et négatif)\n",
        "print(maxlength, \" is maximum sentence length\")\n",
        "print(\n",
        "    txtidx[\n",
        "        0]) # première phrase du dataset, mais sous sa forme numérisée (une séquence d'index)\n",
        "\n",
        "\n",
        "### For the labels\n",
        "labels = th.ones(\n",
        "    [2*NB_SENTENCES]) # avons mis toutes les critiques positives d'abord, puis toutes les critiques négatives\n",
        "labels[\n",
        "    0:NB_SENTENCES] = 0 # première moitié du tableau labels contient des 0 (pour les critiques positives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl15Y5TLc19V",
        "outputId": "73e2f2b6-a121-4ab2-abfc-a0096ab06f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 36,  25, 381,  10,  58,  21,  83]) torch.Size([7])\n",
            "['do', \"n't\", 'miss', 'it', 'if', 'you', 'can']\n",
            "['do', \"n't\", 'miss', 'it', 'if', 'you', 'can']\n"
          ]
        }
      ],
      "source": [
        "def idx2wordlist(idx_array):\n",
        "    l = []\n",
        "    for i in idx_array:\n",
        "        l.append(idx2w[i.item()])\n",
        "    return l\n",
        "print(\n",
        "    txtidx[\n",
        "        0], txtidx[\n",
        "            0].shape) # ce que votre IA va réellement voir\n",
        "\n",
        "print(\n",
        "    idx2wordlist(\n",
        "        txtidx[\n",
        "            0])) # phrase sous sa forme textuelle originale après avoir été numérisée et reconvertie\n",
        "print(\n",
        "    postxt[\n",
        "        1]) # vraie phrase originale telle qu'elle a été chargée et nettoyée depuis le fichier imdb.pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOTdRRANc19W"
      },
      "outputs": [],
      "source": [
        "pack = (txtidx, labels, idx2w)\n",
        "# txtidx (vos phrases transformées en séquences d'index numériques),\n",
        "# labels (les étiquettes positif/négatif correspondantes), et\n",
        "# idx2w (le dictionnaire qui permet de retrouver les mots à partir de leurs index).\n",
        "\n",
        "import pickle # transformer des objets Python complexes en un flux d'octets\n",
        "\n",
        "if True :\n",
        "     pickle.dump(pack, open('imdb-200k', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'instant, nous avons seulement la traduction mot -> index : Ce notebook réalise de la tokenisation, suivie de numérisation.\n"
      ],
      "metadata": {
        "id": "4mE5vbOvqFbN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igOGqfYlqYkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}